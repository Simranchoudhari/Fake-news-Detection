{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update your dataset path accordingly\n",
        "liar_data_path = '/content/train.tsv'\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_liar(path):\n",
        "    return pd.read_csv(path, sep='\\t', header=None, names=[\n",
        "        'id', 'label', 'statement', 'subject', 'speaker', 'speaker_job',\n",
        "        'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts',\n",
        "        'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
        "    ])\n",
        "\n",
        "df = load_liar(liar_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFCsGhjSnbHW",
        "outputId": "91072f94-0ef2-4373-9bc5-f619b98f70f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(df):\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "    return train_df, val_df\n"
      ],
      "metadata": {
        "id": "QkIFsnXGnhY8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def encode_data(texts, labels, max_length=128):\n",
        "    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=max_length, return_tensors=\"tf\")\n",
        "    return encodings, tf.convert_to_tensor(labels)\n",
        "\n",
        "def train_classifier(texts, labels):\n",
        "    encodings, labels_tensor = encode_data(texts, labels)\n",
        "\n",
        "    model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(set(labels)))\n",
        "    model.compile(optimizer=Adam(learning_rate=2e-5),\n",
        "                  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(encodings['input_ids'], labels_tensor, epochs=2, batch_size=8)\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTpVnozZnlCH",
        "outputId": "4f82ad2a-e132-42eb-cb5d-d3780e910b0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate_classifier(model, texts, labels):\n",
        "    encodings, _ = encode_data(texts, labels)\n",
        "    preds = model.predict(encodings['input_ids']).logits\n",
        "    pred_labels = tf.argmax(preds, axis=1).numpy()\n",
        "    acc = accuracy_score(labels, pred_labels)\n",
        "    f1 = f1_score(labels, pred_labels, average='weighted')\n",
        "    return acc, f1\n"
      ],
      "metadata": {
        "id": "mBVYH9JrnvV6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use tf.keras.optimizers.Adam instead of keras.optimizers.Adam\n",
        "def train_classifier(texts, labels):\n",
        "    encodings, labels_tensor = encode_data(texts, labels)\n",
        "\n",
        "    model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(set(labels)))\n",
        "    # Changed here to use tf.keras.optimizers.Adam\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "                  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.fit(encodings['input_ids'], labels_tensor, epochs=2, batch_size=8)\n",
        "    return model"
      ],
      "metadata": {
        "id": "XPUXwEuxn1Kf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the dataset (use either real or fake news statements for training the generator)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['statement'])  # Fit tokenizer on your fake news data\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['statement'])\n",
        "max_seq_length = max([len(seq) for seq in sequences])  # Get max length of sequences\n",
        "sequences_padded = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Now, `sequences_padded` is ready to be used for SeqGAN training\n"
      ],
      "metadata": {
        "id": "jaRMtq-CoT1r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Flatten\n",
        "\n",
        "def build_discriminator(vocab_size, embedding_dim, sequence_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length))\n",
        "    model.add(LSTM(128))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output: Real or Fake (binary classification)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "n99o2MbXhBW2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def train_seqgan(generator, discriminator, real_data, fake_data, batch_size=32):\n",
        "    # Optimizers\n",
        "    g_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
        "    d_optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "    for epoch in range(10000):  # Number of epochs to train\n",
        "        # Train Discriminator (Real + Fake News)\n",
        "        real_labels = tf.ones((batch_size, 1))  # Real labels\n",
        "        fake_labels = tf.zeros((batch_size, 1))  # Fake labels\n",
        "\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            real_output = discriminator(real_data)\n",
        "            fake_output = discriminator(fake_data)\n",
        "            d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_output)\n",
        "            d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "        d_gradients = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "        d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
        "\n",
        "        # Train Generator\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            fake_output = discriminator(fake_data)\n",
        "            g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_output)\n",
        "\n",
        "        g_gradients = g_tape.gradient(g_loss, generator.trainable_variables)\n",
        "        g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: D Loss = {d_loss.numpy()}, G Loss = {g_loss.numpy()}\")\n"
      ],
      "metadata": {
        "id": "ylGPv51RpA96"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_news(generator, num_samples=10):\n",
        "    fake_news = []\n",
        "    for _ in range(num_samples):\n",
        "        generated_sequence = generator.predict(generate_input)  # Your input to the generator here\n",
        "        fake_news.append(tokenizer.sequences_to_texts(generated_sequence))\n",
        "    return fake_news\n"
      ],
      "metadata": {
        "id": "hvGZdDlLpEmT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets -q\n"
      ],
      "metadata": {
        "id": "8eKt15PzpHrC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer\n",
        "import pandas as pd\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "def preprocess_t5_data(df, input_col='statement', label_col='label'):\n",
        "    input_texts = ['classify: ' + str(text) for text in df[input_col]]\n",
        "    label_texts = ['real' if label == 1 else 'fake' for label in df[label_col]]\n",
        "    return input_texts, label_texts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf-INjpBpUBl",
        "outputId": "f498e160-b3d8-4d88-c6b6-9b2740b4e879"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_t5_data(input_texts, target_texts, tokenizer, max_length=128):\n",
        "    input_encodings = tokenizer(input_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    target_encodings = tokenizer(target_texts, padding=True, truncation=True, max_length=16, return_tensors=\"pt\")\n",
        "\n",
        "    input_encodings['labels'] = target_encodings['input_ids']\n",
        "    return input_encodings\n"
      ],
      "metadata": {
        "id": "y7S_UsnopYCg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "voRS_U45qvDY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class T5FakeNewsDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Call preprocess_t5_data to get train_inputs and train_labels\n",
        "train_inputs, train_labels = preprocess_t5_data(df)  # Assuming 'df' is your training DataFrame\n",
        "val_inputs, val_labels = preprocess_t5_data(df)  # Assuming 'df' is your validation DataFrame\n",
        "\n",
        "train_encodings = tokenize_t5_data(train_inputs, train_labels, tokenizer)\n",
        "val_encodings = tokenize_t5_data(val_inputs, val_labels, tokenizer)\n",
        "\n",
        "train_dataset = T5FakeNewsDataset(train_encodings)\n",
        "val_dataset = T5FakeNewsDataset(val_encodings)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # The 'evaluation_strategy' argument has been replaced with 'eval_strategy'.\n",
        "    eval_strategy='epoch',\n",
        "    logging_dir='./logs',\n",
        "    save_total_limit=1,\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "G0ViLyMApfBC",
        "outputId": "f668f977-07c8-4c26-92dd-8e56aa4c907b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-14-e8d966a2c090>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1195' max='2560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1195/2560 1:21:52 < 1:33:40, 0.24 it/s, Epoch 0.93/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2560' max='2560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2560/2560 3:38:24, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-e8d966a2c090>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2560, training_loss=0.08641776070189736, metrics={'train_runtime': 13114.5158, 'train_samples_per_second': 1.562, 'train_steps_per_second': 0.195, 'total_flos': 692950023536640.0, 'train_loss': 0.08641776070189736, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_t5(text):\n",
        "    input_text = \"classify: \" + text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    output = model.generate(input_ids)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example\n",
        "print(predict_t5(\"The earth is flat and the moon is made of cheese.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeZk0Bo5piIH",
        "outputId": "cc85d8ab-35ff-467a-9c80-70844ce9c322"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_t5(text, model, tokenizer):\n",
        "    input_text = \"classify: \" + text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    output = model.generate(input_ids)\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "n3H2_15UdZmX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate_t5(model, tokenizer, val_df):\n",
        "    predictions = []\n",
        "    for text in val_df['statement']:\n",
        "        pred = predict_t5(text, model, tokenizer)\n",
        "        predictions.append(pred.strip().lower())\n",
        "\n",
        "    true_labels = ['real' if l == 1 else 'fake' for l in val_df['label']]\n",
        "\n",
        "    acc = accuracy_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions, pos_label='real')\n",
        "\n",
        "    print(f\"T5 Validation Accuracy: {acc:.4f}\")\n",
        "    print(f\"T5 F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "8kdLbr1wdbId"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(df):\n",
        "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "    return train_df, val_df\n",
        "\n",
        "# Call split_dataset to create train_df and val_df\n",
        "train_df, val_df = split_dataset(df)  # Assuming 'df' is your original DataFrame\n",
        "\n",
        "# ... (Rest of the code, including evaluate_t5 and its call)"
      ],
      "metadata": {
        "id": "-z9Bj_MVdxCL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_t5(model, tokenizer, val_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4DGwxmAde-n",
        "outputId": "efdbfdcb-b6de-493e-df43-b0703c94bab7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5 Validation Accuracy: 1.0000\n",
            "T5 F1 Score: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/t5_fake_news_classifier\")\n",
        "tokenizer.save_pretrained(\"/content/t5_fake_news_classifier\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOQRe1fqd0Zn",
        "outputId": "fa7d0395-e474-4ab9-ee9d-4574e7491d7a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/t5_fake_news_classifier/tokenizer_config.json',\n",
              " '/content/t5_fake_news_classifier/special_tokens_map.json',\n",
              " '/content/t5_fake_news_classifier/spiece.model',\n",
              " '/content/t5_fake_news_classifier/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}